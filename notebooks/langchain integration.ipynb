{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"ministral-3:3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f0aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Retrieval-Augmented Generation (RAG)** is a technique used in **large language models (LLMs)** to improve accuracy, context awareness, and reduce hallucinations by combining **retrieval** (finding relevant information) with **generation** (producing text). It is particularly useful when the model’s internal knowledge cutoff (e.g., 2023-10-01 for me) is insufficient for up-to-date or domain-specific queries.\n",
      "\n",
      "---\n",
      "\n",
      "### **Core Components of RAG**\n",
      "RAG typically consists of **three main parts**:\n",
      "\n",
      "1. **Retrieval Module**\n",
      "   - Fetches relevant information from an external knowledge base (e.g., a database, corpus, or vector store).\n",
      "   - Uses techniques like:\n",
      "     - **Keyword matching** (simple term-based search).\n",
      "     - **Semantic search** (e.g., embeddings + cosine similarity, like in **BM25** or **FAISS**).\n",
      "     - **Hybrid approaches** (combining keyword + semantic search).\n",
      "\n",
      "2. **Augmentation**\n",
      "   - The retrieved documents are **contextualized** (e.g., concatenated with the user’s query) to provide richer input to the LLM.\n",
      "   - Example: If the query is *\"What happened in 2025?\"*, retrieved documents from 2025-10-01 onward are appended to the prompt.\n",
      "\n",
      "3. **Generation Module**\n",
      "   - The LLM (e.g., me) processes the augmented input (query + retrieved context) to generate a response.\n",
      "   - The model may use **attention mechanisms** to focus on relevant parts of the input.\n",
      "\n",
      "---\n",
      "\n",
      "### **How RAG Works in Practice**\n",
      "Here’s a step-by-step example:\n",
      "\n",
      "1. **User Query**: *\"Who won the Nobel Prize in Physics in 2025?\"*\n",
      "   - The system checks its internal knowledge cutoff (2023-10-01) and realizes it’s outdated.\n",
      "\n",
      "2. **Retrieval**:\n",
      "   - The system searches a **vector database** (e.g., using embeddings of news articles, papers, or official sources) for documents about 2025 Nobel Prizes.\n",
      "   - Example retrieved snippet:\n",
      "     > *\"The 2025 Nobel Prize in Physics was awarded to Dr. Alice Chen and Dr. Bob Smith for their work on quantum computing.\"*\n",
      "\n",
      "3. **Augmentation**:\n",
      "   - The retrieved snippet is appended to the prompt:\n",
      "     > *\"Who won the Nobel Prize in Physics in 2025? [Retrieved context: ...]\"*\n",
      "\n",
      "4. **Generation**:\n",
      "   - The LLM (e.g., me) generates the answer using the augmented input, ensuring accuracy for recent events.\n",
      "\n",
      "---\n",
      "\n",
      "### **Why RAG Improves LLMs**\n",
      "- **Reduces Hallucinations**: Avoids fabricating outdated or incorrect information.\n",
      "- **Supports Domain-Specific Knowledge**: Useful for niche topics (e.g., legal, medical, or recent scientific updates).\n",
      "- **Handles Long-Term Context**: Unlike traditional LLMs, RAG can reference external data without relying solely on internal knowledge.\n",
      "- **Scalability**: Works with large datasets (e.g., Wikipedia, news archives, or proprietary databases).\n",
      "\n",
      "---\n",
      "\n",
      "### **Challenges of RAG**\n",
      "- **Latency**: Retrieval can be slower than direct generation.\n",
      "- **Context Window Limits**: Some RAG systems may struggle with very long queries or documents.\n",
      "- **Data Quality**: Poorly curated or irrelevant retrieved documents can degrade performance.\n",
      "- **Computational Cost**: Storing and indexing large datasets requires significant resources.\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Architectures**\n",
      "1. **Passive Retrieval-Augmented Generation (PRAG)**:\n",
      "   - Simple: Retrieve documents and pass them to the LLM as-is (no fine-tuning).\n",
      "   - Example: Using a **BM25** search over a corpus.\n",
      "\n",
      "2. **Active Learning**:\n",
      "   - The LLM can dynamically query the retrieval system for additional context during generation (e.g., via iterative prompts).\n",
      "\n",
      "3. **Hybrid Models**:\n",
      "   - Combines RAG with **fine-tuning** (e.g., training the LLM to better use retrieved data).\n",
      "\n",
      "---\n",
      "\n",
      "### **Tools/Libraries for RAG**\n",
      "- **Vector Databases**: FAISS, Weaviate, Pinecone, Milvus.\n",
      "- **Retrieval Methods**: Sentence Transformers (for embeddings), BM25.\n",
      "- **LLM Integration**: Hugging Face Transformers, LangChain (for modular RAG pipelines).\n",
      "\n",
      "---\n",
      "### **When to Use RAG?**\n",
      "- You need **up-to-date information** (e.g., 2025 events).\n",
      "- The topic is **domain-specific** (e.g., legal cases, medical research).\n",
      "- You want to **reduce hallucinations** in factual queries.\n",
      "\n",
      "---\n",
      "### **Limitations**\n",
      "- **Not a silver bullet**: RAG still relies on the quality of the retrieved data.\n",
      "- **Overhead**: Requires infrastructure for storage/retrieval (e.g., a database).\n",
      "- **Context Window**: Some RAG systems can’t handle extremely long queries.\n",
      "\n",
      "---\n",
      "Would you like a deeper dive into a specific aspect (e.g., implementation with code, or comparison to other methods like **Retrieval-Augmented Inference (RAI)**)?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"How does RAG work?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe454f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
